{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdBp8aZAhB2z"
   },
   "source": [
    "# Kenyan Accident Data ETL\n",
    "\n",
    "**Tasks:**:\n",
    "1.   Web Scraping: Get data links with \"Beautiful Soup\"\n",
    "2.   Import, manipulate and store the data with \"Pandas\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jNoxRd92FtVJ"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries/packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, re\n",
    "from dateutil.parser import parse\n",
    "import mimetypes\n",
    "\n",
    "# Suppress UserWarning in 'openpyxl' module\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WOD0jEilFzlQ"
   },
   "outputs": [],
   "source": [
    "def scrape_data_links(host, page_url):\n",
    "\n",
    "    # Get page content\n",
    "    webpage = requests.get(page_url)\n",
    "\n",
    "    # Convert content from bytes to HTML\n",
    "    soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "\n",
    "    # Extract report/data paths\n",
    "    link_tags = soup.find_all('a')\n",
    "\n",
    "    # Obtain complete data links from host address and data paths\n",
    "    data_links = []\n",
    "\n",
    "    for tag in link_tags:\n",
    "        # is_excel_file = tag.get('href').endswith('.xlsx')\n",
    "        # is_report = 'report' in tag.text.lower()\n",
    "        # is_data_path = is_report and is_excel_file\n",
    "\n",
    "        pattern = r\".+fatal.+report.+\\.xlsx\"\n",
    "        is_data_path = re.search(pattern, tag.get('href').lower())\n",
    "\n",
    "\n",
    "        if is_data_path:\n",
    "            data_links.append(archive_host + tag.get('href'))\n",
    "\n",
    "\n",
    "    return data_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bGhj6hghIYJE"
   },
   "outputs": [],
   "source": [
    "# def extract_data(data_link):\n",
    "#     data_columns = ['S/NO', 'TIME 24 HOURS', 'BASE/SUB BASE', 'COUNTY', 'ROAD', 'PLACE', 'MV INVOLVED', 'BRIEF ACCIDENT DETAILS', 'NAME OF VICTIM', 'GENDER', 'AGE', 'CAUSE CODE', 'VICTIM', 'NO.']\n",
    "\n",
    "#     # Skip the summary footer\n",
    "#     data = pd.read_excel(data_link, skipfooter=6, names=data_columns)\n",
    "\n",
    "#     # Extract date from report title(string)\n",
    "#     title_str = data.iloc[0, 0]\n",
    "#     date_fmt = \"%m/%d/%Y\"\n",
    "#     file_date = \"\"\n",
    "\n",
    "\n",
    "#     if title_str:\n",
    "#       file_date = parse(title_str, fuzzy=True).strftime(date_fmt)\n",
    "\n",
    "#     # Drop empty rows\n",
    "#     data.dropna(how='all', inplace=True)\n",
    "\n",
    "#     # Add report date\n",
    "#     data['DATE'] = file_date\n",
    "\n",
    "#     # Return data excluding the first two irrelevant rows\n",
    "#     return data.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "p3LGLL_mVOmL"
   },
   "outputs": [],
   "source": [
    "def extract_date_from_string(input_string):\n",
    "    # Define a regular expression pattern to match the date format\n",
    "    date_pattern = r'(\\d{1,2})/(\\d{1,2})/(\\d{4})'\n",
    "\n",
    "    # Search for the date pattern in the input string\n",
    "    date_match = re.search(date_pattern, input_string)\n",
    "\n",
    "    if date_match:\n",
    "        # Extract and return the matched date\n",
    "        extracted_date = date_match.group()\n",
    "        return extracted_date\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dsnAvIg3UMoQ"
   },
   "outputs": [],
   "source": [
    "def extract_data(data_file):\n",
    "    data_columns = ['S/NO', 'TIME 24 HOURS', 'BASE/SUB BASE', 'COUNTY', 'ROAD', 'PLACE', 'MV INVOLVED', 'BRIEF ACCIDENT DETAILS', 'NAME OF VICTIM', 'GENDER', 'AGE', 'CAUSE CODE', 'VICTIM', 'NO.']\n",
    "\n",
    "    # Skip the summary footer\n",
    "    data = pd.read_excel(data_file, skipfooter=6, names=data_columns)\n",
    "\n",
    "    # Extract date from report title(string)\n",
    "    title_str = data.iloc[0, 0]\n",
    "    file_date = \"\"\n",
    "\n",
    "\n",
    "    if title_str:\n",
    "        file_date = extract_date_from_string(title_str)\n",
    "\n",
    "    # Drop empty rows\n",
    "    data.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Add report date\n",
    "    data['DATE'] = file_date\n",
    "\n",
    "    # Return data excluding the first two irrelevant rows\n",
    "    return data.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YwR2mXTzGo-v"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(links_list, save_dataset=False, data_path='dataset.csv'):\n",
    "    file_count = 0\n",
    "    no_file_count = 0\n",
    "    all_columns = ['DATE', 'TIME 24 HOURS', 'BASE/SUB BASE', 'COUNTY', 'ROAD', 'PLACE', 'MV INVOLVED', 'BRIEF ACCIDENT DETAILS', 'GENDER', 'AGE', 'CAUSE CODE', 'VICTIM', 'NO.']\n",
    "    valid_links = []\n",
    "\n",
    "    # Create Empty dataframe for dataset\n",
    "    dataset = pd.DataFrame(columns=all_columns)\n",
    "\n",
    "    for link in links_list:\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "\n",
    "            # Get the content type from the response headers\n",
    "            content_type = response.headers.get('content-type')\n",
    "\n",
    "            # Use mimetypes module to guess the file extension based on content type\n",
    "            not_excel_file = mimetypes.guess_extension(content_type) != '.xlsx'\n",
    "\n",
    "            if not_excel_file:\n",
    "                # print(\"not ok\")\n",
    "                no_file_count += 1\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                file_count += 1\n",
    "                new_data = extract_data(response.content)\n",
    "\n",
    "                dataset = pd.concat([dataset, new_data])\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Exception occurred: {e} \\n Skipping to next item.\")\n",
    "            print(f\"Exception occurred! \\n Skipping to next item.\")\n",
    "            continue\n",
    "\n",
    "        finally:\n",
    "            print(f\"Loop {links_list.index(link) + 1} complete\")\n",
    "\n",
    "\n",
    "    print(f\"Total Invalid Files: {no_file_count}\")\n",
    "    print(f\"Total Valid Files: {file_count}\")\n",
    "\n",
    "    # Save dataset as CSV\n",
    "    dataset.to_csv(\"dataset.csv\", header=True, index=False)\n",
    "\n",
    "    if save_dataset:\n",
    "        # Save dataset as CSV\n",
    "        dataset.to_csv(data_path, header=True, index=False)\n",
    "\n",
    "\n",
    "    return dataset, valid_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WL5oemgLpCOR"
   },
   "outputs": [],
   "source": [
    "def load_text_as_list(file_path):\n",
    "    data_list = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Convert each line to a string and append to the list\n",
    "            data_list.append(str(line.strip()))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NLDG4AyBsJZ0"
   },
   "outputs": [],
   "source": [
    "def save_list_as_text(data, file_path='data.txt'):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data:\n",
    "            file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ntg6ipx1Ghvj"
   },
   "outputs": [],
   "source": [
    "archive_host = \"https://web.archive.org/web/20181216075237/http://www.ntsa.go.ke/\"\n",
    "archive_URL = \"https://web.archive.org/web/20181216075237/http://www.ntsa.go.ke/index.php?option=com_content&view=article&id=237\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oywFedUyGj_4",
    "outputId": "1422617a-aa2b-4d6d-919b-7249488279fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Links: 44\n"
     ]
    }
   ],
   "source": [
    "# Get links to data direct from archive\n",
    "# data_links = scrape_data_links(archive_host, archive_URL)\n",
    "\n",
    "# Get links to data \"scraped and verified\" from archive\n",
    "data_links = load_text_as_list('valid_links.txt')\n",
    "\n",
    "print(f\"Total Links: {len(data_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryfRYO77TqFb"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "dataset, valid_links = generate_dataset(data_links, save_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hsKSwfIz9vq"
   },
   "outputs": [],
   "source": [
    "# Save_valid_links\n",
    "save_list_as_text(valid_links, file_path='valid_links.txt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
